{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316caeaa",
   "metadata": {},
   "source": [
    "### Note: Not all problems in this Jupyter Notebook are meant to be solved with Python. All problems are here to keep things organized in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe162c5",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "In order of most to least requested, the top six topics for our section to review were:\n",
    "\n",
    "1. SVD\n",
    "2. (tie) Change of basis\n",
    "2. (tie) Linear dynamical systems\n",
    "4. Markov stochastic processes\n",
    "5. (tie) Least squares fit\n",
    "5. (tie) Projections\n",
    "\n",
    "In order of most to least requested, the specific problems requested were (organzied into general types):\n",
    "\n",
    "\n",
    "1. Fundamental subspaces of a matrix (e.g., rank, nullity, uniqueness of solutions) (8)\n",
    "2. Gram-Schmidt (7)\n",
    "3. Least squares fit and pseudoinverse (6)\n",
    "3. General SVD (6)\n",
    "5. Setting up a Markov Matrix (5)\n",
    "5. Bases (note: the questions suggested often mixed things up with other ideas. This is a good sign that we need to review the language and ideas around bases!) (5)\n",
    "5. Quiz problems (5)\n",
    "8. Change of basis (4)\n",
    "8. Linear dynamical systems (e.g., setup, coding, diagonalization) (4)\n",
    "10. Properties of orthogonal matrices (3)\n",
    "10. Plotting (3)\n",
    "12. Projection onto a subspace and projection matrices (2)\n",
    "12. Coding problems (2)\n",
    "14. General diagonalization (1)\n",
    "14. Matching transformations to their eigenvalues/eigenvectors (1)\n",
    "14. Gauss-Jordan by hand (1)\n",
    "14. Vector spaces (1)\n",
    "14. General eigenvectors and eigenvalues (1)\n",
    "14. Inner products (1)\n",
    "\n",
    "**DISCLAIMER**: This review is customized to our section's responses for what you want to review. There are many topics that **WILL** be on the final that are not covered in this review. For more practice problems from sections not covered here, check the Final Review assignments from last year under In-class Assignments on D2L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6144346c",
   "metadata": {},
   "source": [
    "---\n",
    "##### This question reviews Markov stochastic processes, linear dynamical systems, eigenvalues/eigenvectors, diagonalization, and change of basis.\n",
    "1. Consider the following (fictional) situation. A study found that freshmen at MSU spend most of their time in one of three places: their dorm, the dining hall, or in class. After one hour, on average, students\n",
    "    - in their dorm had a 35% chance of going to the dining hall, and a 20% chance of going to class (staying in their dorms the rest of the time)\n",
    "    - in the dining hall had a 70% chance of going to their dorm, and a 10% chance of going to class\n",
    "    - in class had a 50% chance of going to their dorm and a 40% chance of going to the dining hall.\n",
    "    1. Write a Markov transition matrix to represent this situation. Let the first state be \"in dorm\", the second state be \"in dining hall\", and the third state be \"in class\".\n",
    "    2. Explain mathematically why your matrix is a transition matrix.\n",
    "    3. There are approximately 10,000 freshman at MSU. Find the steady state of approximately how many freshman are where on campus.\n",
    "    4. Assume that all students start in their dorm. How many students are in each location after two hours? After the first day? **Important:** Solve this problem by repeatedly applying $A$ via **diagonalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3989f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "import numpy as np\n",
    "A = np.array([[0.45, 0.7, 0.5], [0.35, 0.2, 0.4], [0.2, 0.1, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506aafe",
   "metadata": {},
   "source": [
    "A. $$A = \n",
    "\\begin{bmatrix}\n",
    "0.45 & 0.7 & 0.5 \\\\\n",
    "0.35 & 0.2 & 0.4 \\\\\n",
    "0.2 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Remember that each row and column corresponds to the given states in order. Each entry represents how people move **from** the column state **into** the row state. You can remember this by thinking about how matrix multiplication works with the state vector.\n",
    "\n",
    "E.g., 40% of students go from class to dining hall. This is moving from third state into second state, so should go in column 3, row 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f311f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B\n",
    "np.allclose(np.sum(A, axis = 0), [1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa981221",
   "metadata": {},
   "source": [
    "B. Columns all sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b706f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5354.33070866+0.j]\n",
      " [3110.23622047+0.j]\n",
      " [1535.43307087+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# C\n",
    "d, C = np.linalg.eig(A)\n",
    "# Tells us which eigenvalue is equal to 1\n",
    "steady_state_index = np.isclose(d, 1)\n",
    "# We then use this to index the columns of the eigenvector matrix\n",
    "steady_state_vec = C[:, np.isclose(d, 1)]\n",
    "# steady_state_vec is normalized. But we want its entries to sum\n",
    "# to 10,000. So we divide by the sum of the entries and multiply\n",
    "# by 10,000.\n",
    "steady_state = 10000 * steady_state_vec / sum(steady_state_vec)\n",
    "print(steady_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4898d948",
   "metadata": {},
   "source": [
    "Ignoring the complex part (this is complex roundoff error in `numpy`), we see that there are about 5354 students in their dorm, 3110 students in the dining hall, and 1535 students in class at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6d2d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5475.-1.0658141e-14j 3075.+1.0658141e-14j 1450.+0.0000000e+00j]\n",
      "[5354.33070866+1.54074396e-33j 3110.23622047+0.00000000e+00j\n",
      " 1535.43307087-2.31111593e-33j]\n",
      "[5475.-8.67361738e-15j 3075.+4.33680869e-15j 1450.+4.33680869e-15j]\n",
      "[5354.33070866+1.88079096e-33j 3110.23622047+0.00000000e+00j\n",
      " 1535.43307087+0.00000000e+00j]\n"
     ]
    }
   ],
   "source": [
    "# D\n",
    "initial = np.array([10000, 0, 0]).T  # Column vector\n",
    "\n",
    "## Version in terms of change of basis\n",
    "# Change of basis into basis of eigenvectors\n",
    "initial_eig_basis = np.linalg.inv(C) @ initial\n",
    "# Do repeated application of A in eigenvector basis\n",
    "two_hours_eig_basis = np.diag(d ** 2) @ initial_eig_basis\n",
    "# Change back to standard basis\n",
    "two_hours = C @ two_hours_eig_basis\n",
    "print(two_hours)\n",
    "# Do same for 1 day\n",
    "one_day_eig_basis = np.diag(d ** 24) @ initial_eig_basis\n",
    "one_day = C @ one_day_eig_basis\n",
    "print(one_day)\n",
    "\n",
    "## Version using powers in terms of diagonalization\n",
    "two_hours = C @ np.diag(d ** 2) @ np.linalg.inv(C) @ initial\n",
    "print(two_hours)\n",
    "one_day = C @ np.diag(d ** 24) @ np.linalg.inv(C) @ initial\n",
    "print(one_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4712778",
   "metadata": {},
   "source": [
    "After two hours, there are about 5475 students in their dorm, 3075 in the dining hall, and 1450 in class. After a full day, the system has reached the steady state given in part (C).\n",
    "\n",
    "Notice that we solved this in two ways that are equivalent. The first was using a change of basis of the initial vector into the basis of eigenvectors. Then we applied the diagonal form of $A$ (i.e., how $A$ works on vectors' representations in the eigenbasis) for as many hours as necessary. Then we changed back to the standard basis.\n",
    "\n",
    "The second way is using the \"power\" property of diagonalization:\n",
    "$$\n",
    "A = CDC^{-1} \\implies A^n = (CDC^{-1})(CDC^{-1})\\cdots(CDC^{-1}) = CD^nC^{1}\n",
    "$$\n",
    "and the fact that the $n$th power of a diagonal matrix is the same as taking the $n$th power of each diagonal entry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb098f",
   "metadata": {},
   "source": [
    "---\n",
    "##### This question reviews the SVD, the pseudoinverse, and orthogonal matrices\n",
    "2. Given a matrix $A \\in \\mathbb{R}^{m \\times n}$:\n",
    "    1. define its SVD. Be specific with how many matrices are involved, the size of each matrix, properties of each matrix, and how the matrices multiply to equal $A$.\n",
    "    2. Given the `numpy` array `A`, write the code to compute its SVD (Hint: make sure all three **matrices** are accounted for).\n",
    "    3. Write the formula for the pseudoinverse of $A$ in terms of its SVD.\n",
    "    4. Show that if $A$ is invertible, then your answer to (B) is equal to the inverse of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f12d36",
   "metadata": {},
   "source": [
    "A.\n",
    "\n",
    "The SVD of $A$ is the formula\n",
    "$$A = U \\Sigma V^T,$$\n",
    "where\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is orthogonal.\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal. The diagonal entries are the *singular values*, $\\sigma_1, \\ldots, \\sigma_{\\min(m,n)}$ of $A$. Only $\\mathrm{rank}(A)$ many singular values are nonzero.\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b3a0eb",
   "metadata": {},
   "source": [
    "B.\n",
    "\n",
    "```python\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "# Turn singular values into matrix\n",
    "Sigma = np.zeros(shape(A))\n",
    "Sigma[:len(s), :len(s)] = np.diag(s)\n",
    "np.allclose(U @ Sigma @ Vt, A)  # Just a check, not necessary\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ac670",
   "metadata": {},
   "source": [
    "C.\n",
    "\n",
    "$$A^+ = V \\Sigma^+ U^\\top$$\n",
    "where\n",
    "$$\\Sigma^+ =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_1} &  &  & & & &  \\\\\n",
    " & \\frac{1}{\\sigma_2} & & & & &  \\\\\n",
    "& & \\ddots & & & & \\\\\n",
    " &  &  & \\frac{1}{\\sigma_r} &  & &  \\\\\n",
    " &  &  &  & 0 & &  \\\\\n",
    " &  &  &  &  & \\ddots &  \\\\\n",
    " &  &  &  &  &  & 0 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{n \\times m}$$\n",
    "(all empty entries are zero).\n",
    "\n",
    "In `numpy`, we could make $\\Sigma^+$ by doing\n",
    "```python\n",
    "_, s, _ = np.linalg.svd(A)\n",
    "# Important! shape(Sigma_plus) == shape(A.T), not shape(A)\n",
    "Sigma_plus = np.zeros(shape(A.T))\n",
    "s = s[~np.isclose(s, 0)]  # Only use nonzero singular values\n",
    "Sigma_plus[:len(s), :len(s)] = np.diag(1 / s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fd322",
   "metadata": {},
   "source": [
    "D.\n",
    "\n",
    "We can show this by multiplying $A$ and $A^+$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "AA^+\n",
    "    &= (U \\Sigma V^\\top) (V \\Sigma^+ U^\\top) \\\\\n",
    "    &= U \\Sigma \\Sigma^+ U^\\top \\quad\\text{(since $V$ is orthogonal)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $A$ is invertible, it is square (and therefore so are $\\Sigma$ and $\\Sigma^+$) and full rank. This means $A$ has no zero singular values, so $\\Sigma^+$ looks like\n",
    "$$\n",
    "\\Sigma^+=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_1} & & & \\\\\n",
    "& \\frac{1}{\\sigma_2} & & \\\\\n",
    "& & \\ddots & \\\\\n",
    "& & & \\frac{1}{\\sigma_n}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "When we multiply $\\Sigma \\Sigma^+$, we multiply the diagonal entries and therefore get the identity. So\n",
    "$$AA^+ = U \\Sigma \\Sigma^+ U^\\top = U U^\\top = I$$\n",
    "again using the fact that $U$ is orthogonal. Thus, $A^+$ must be the $A^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba74df6",
   "metadata": {},
   "source": [
    "---\n",
    "##### This question reviews span, linear independence, basis vectors, Gram-Schmidt, and projection onto subspaces\n",
    "\n",
    "3. Consider the plane defined by the equation\n",
    "$$x_1 - 2 x_2 = x_3 \\quad \\text{ for any $x_1, x_2 \\in \\mathbb{R}$}.$$\n",
    "    1. Use the equation to come up with a spanning set for this plane.\n",
    "    2. Check that this spanning set is basis for the plane.\n",
    "    3. Perform the Gram-Schmidt procedure to create an orthonormal basis for the plane.\n",
    "    4. Project the vector $$b = \\begin{bmatrix} 3 \\\\1\\\\ -1\\end{bmatrix}$$ onto the plane using your answer from part (C)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3d55c",
   "metadata": {},
   "source": [
    "A.\n",
    "\n",
    "Any vector that on the plane must have the form\n",
    "$$\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_1 - 2x_2\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{ for any $x_1, x_2 \\in \\mathbb{R}$}.\n",
    "$$\n",
    "If we split the $x_1$ and $x_2$ parts, we can write this as a linear combination of two vectors:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_1 - 2x_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "0 \\\\\n",
    "x_1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "x_2 \\\\\n",
    "- 2x_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "x_1 \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "x_2\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "- 2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Thus, the vectors\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ 0 \\\\ 1\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 1 \\\\ -2\n",
    "\\end{bmatrix}\\right\\}\n",
    "$$\n",
    "are a spanning set for the plane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d1f1e",
   "metadata": {},
   "source": [
    "B.\n",
    "\n",
    "To show that these vectors form a basis for the plane, we need to check two things\n",
    "1. They span the plane\n",
    "2. They are linearly independent\n",
    "\n",
    "We already know they span the plane from (A). Just to be very clear, we showed that any vector on the plane can be written as the linear combination\n",
    "$$\n",
    "x_1 \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "x_2\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "- 2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "This means that the two vectors span the plane.\n",
    "\n",
    "To show that they are linearly independent, we have to check that the only linear combination that equals the all-zero vector is where the coefficients are all zero. That is, we need to show that if\n",
    "$$\n",
    "x_1 \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "+\n",
    "x_2\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "- 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "then $x_1$ and $x_2$ have to be zero. This can be accomplished a few different ways:\n",
    "- If we look at the first line of this equation, we see that $x_1$ would have to equal zero. The second line tells us that $x_2$ has to equal zero. So we've shown that $x_1$ and $x_2$ are equal to zero.\n",
    "- When we have two vectors, it suffices to show that neither vector is a scalar multiple of the other. This is true, since any scalar multiple of the first vector will have a 0 in its second entry. But the second spanning vector has a 1 there.\n",
    "- We could do Gauss-Jordan with these vectors as rows. If we end up with a row of all zeros in the reduced row echelon form, the vectors are **not** linearly independent (i.e., they are linearly dependent):\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & -2\n",
    "\\end{bmatrix}...$$\n",
    "well, this matrix is already in RREF, so we know they're linearly independent (this is what makes that first option so easy too).\n",
    "\n",
    "**Note**: A common technique for showing that vectors are linearly indepedent is to make the matrix with these vectors as columns, and show that that matrix is invertible (by, e.g., showing it has a nonzero determinant, showing its RREF is the identity, showing it is full rank, actually computing its inverse, etc).\n",
    "\n",
    "This doesn't work when you're trying to show that a set of vectors is a basis of a subspace with lower dimension than the length of the vectors! In our case, we have 2 vectors that are length 3. If we made that matrix, it would be a $3\\times 2$ matrix. This isn't square, so it can't have an inverse!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d918b09",
   "metadata": {},
   "source": [
    "C.\n",
    "\n",
    "The Gram-Schmidt algorithm takes a list of linearly independent vectors $\\{v_1, v_2, \\ldots, v_n\\}$ as input and returns an orthonormal list of vectors $\\{w_1, w_2, \\ldots w_n\\}$.\n",
    "\n",
    "- $w_1 \\leftarrow \\frac{v_1}{\\|v_1\\|}$  $\\quad$\n",
    "- for $i = {2, 3, \\ldots, n}$\n",
    "    - $w \\leftarrow v_i$\n",
    "    - for $j = {1, 2, \\ldots, i}$\n",
    "        - $w \\leftarrow w - (v_i \\cdot w_j) w_j$\n",
    "    - $w_i \\leftarrow \\frac{w}{\\| w \\|}$\n",
    "    \n",
    "    \n",
    "In simple language,\n",
    "1. Normalize the first vector\n",
    "2. Subtract the projection of the next vector onto the previous vectors from that vector\n",
    "3. Normalize that vector\n",
    "4. Repeat steps 2-3 for all remaining vectors\n",
    "\n",
    "In this case\n",
    "1. Normalize the first vector:\n",
    "$$\n",
    "w_1 = \\frac{v_1}{\\|v_1\\|} =\n",
    "\\frac{1}{\\sqrt{1^1 + 0^2 + 1^1}}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}$$\n",
    "2. Subtract the projection of the next vector onto the previous vectors from that vector:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w \n",
    "    &= v_2 - \\mathrm{proj}_{w_1}(v_2) \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    - 2\n",
    "    \\end{bmatrix} -\n",
    "    \\left(\n",
    "    \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    - 2\n",
    "    \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}}\n",
    "    \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "    \\end{bmatrix}\n",
    "    \\right)\n",
    "    \\frac{1}{\\sqrt{2}}\n",
    "    \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    - 2\n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    -1\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "3. Normalize that vector\n",
    "$$\n",
    "w_2 = \\frac{w}{\\|w\\|} = \\frac{1}{\\sqrt{1^1 + 1^2 + (-1)^2}}\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    -1\n",
    "\\end{bmatrix}\n",
    "= \\frac{1}{\\sqrt{3}}\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "4. Repeat steps 2-3 for all remaining vectors:\n",
    "We have no remaining vectors, so we're done!\n",
    "\n",
    "Our final answer is that\n",
    "$$\n",
    "\\{w_1, w_2\\} = \n",
    "\\left\\{ \\frac{1}{\\sqrt{2}}\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}, \\frac{1}{\\sqrt{3}}\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    -1\n",
    "\\end{bmatrix}\\right\\}$$\n",
    "is an orthonormal basis for the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08cb3759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678  0.57735027]\n",
      " [ 0.          0.57735027]\n",
      " [ 0.70710678 -0.57735027]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# C\n",
    "V = np.array([[1, 0, 1], [0, 1, -2]]).T  # Basis as columns of array\n",
    "\n",
    "def proj(v1, v2):\n",
    "    # Project the column vector v2 onto the column vector v1\n",
    "    return (v2.T @ v1) * v1\n",
    "\n",
    "def gram_schmidt(V):\n",
    "    # Convert a basis of column vectors in array to orthonormal basis\n",
    "    W = np.zeros(V.shape)\n",
    "    for i in range(W.shape[1]): \n",
    "        W[:, i] = V[:, i]\n",
    "        for j in range(i):\n",
    "            # Subtract projection of v_i onto previous vectors\n",
    "            W[:, i] = W[:, i] - proj(W[:, j], V[:, i])\n",
    "        # Normalize vector\n",
    "        W[:, i] = W[:, i] / np.linalg.norm(W[:, i])\n",
    "    return W\n",
    "\n",
    "W = gram_schmidt(V)\n",
    "print(W)\n",
    "# Check that it's equal to vectors computed by hand\n",
    "w1 = np.array([[1, 0, 1]]).T / np.sqrt(2)\n",
    "w2 = np.array([[1, 1, -1]]).T / np.sqrt(3)\n",
    "print(np.allclose(W, np.hstack((w1, w2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe8af5",
   "metadata": {},
   "source": [
    "D.\n",
    "\n",
    "Let $W$ be the plane. To project a vector onto a subspace, we add the projection onto each basis vector.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{proj}_W(b) \n",
    "    &= \\mathrm{proj}_{w_1}(b) + \\mathrm{proj}_{w_2}(b)\\\\\n",
    "    &= (b \\cdot w_1) w_1 + (b \\cdot w_2) w_2 \\\\\n",
    "    &= \\left(\n",
    "    \\begin{bmatrix} 3 \\\\1\\\\ -1\\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\frac{1}{\\sqrt{2}}\n",
    "    \\begin{bmatrix} 1 \\\\0 \\\\1\\end{bmatrix}\n",
    "    \\right)\n",
    "    \\frac{1}{\\sqrt{2}}\n",
    "    \\begin{bmatrix} 1 \\\\0 \\\\1\\end{bmatrix}\n",
    "    + \\left(\n",
    "    \\begin{bmatrix} 3 \\\\1\\\\ -1\\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\frac{1}{\\sqrt{3}}\n",
    "    \\begin{bmatrix} 1 \\\\1 \\\\-1\\end{bmatrix}\n",
    "    \\right)\n",
    "    \\frac{1}{\\sqrt{3}}\n",
    "    \\begin{bmatrix} 1 \\\\1 \\\\-1\\end{bmatrix}\\\\\n",
    "    &= \\begin{bmatrix} 1 \\\\0 \\\\1\\end{bmatrix}\n",
    "    +\n",
    "    \\frac{5}{3}\n",
    "    \\begin{bmatrix} 1 \\\\1 \\\\-1\\end{bmatrix}\\\\\n",
    "    &=\n",
    "    \\frac{1}{3}\n",
    "    \\begin{bmatrix}8 \\\\5\\\\-2\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8313ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.66666667]\n",
      " [ 1.66666667]\n",
      " [-0.66666667]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# D\n",
    "b = np.array([[3, 1, -1]]).T\n",
    "\n",
    "# Shortcut for computing dot products, then taking linear combination\n",
    "b_proj = W @ W.T @ b\n",
    "\n",
    "# An alternative is\n",
    "b_proj = np.zeros((W.shape[0], 1))\n",
    "for i in range(W.shape[1]):\n",
    "        b_proj[:, 0] = b_proj[:, 0] + proj(W[:, i], b)\n",
    "        \n",
    "print(b_proj)\n",
    "# Check that it's equal to projection computed by hand\n",
    "print(np.allclose(b_proj, np.array([[8, 5, -2]]).T / 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864650ae",
   "metadata": {},
   "source": [
    "---\n",
    "##### This question reviews row space, column space, null space, rank, nullity, Gauss-Jordan and uniqueness of solutions\n",
    "\n",
    "4. Consider the matrix\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & -1 & -2 & 4\\\\\n",
    "-2 & 1 & 3 & -7\\\\\n",
    "1 & 0 & -1 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "    1. Find a basis for the row space of $A$.\n",
    "    2. Find a basis for the column space of $A$.\n",
    "    3. Find a basis for the range of $A$.\n",
    "    4. What is $\\mathrm{rank}(A)$?\n",
    "    5. What is the dimension of the null space of $A$?\n",
    "    6. What is the dimension of the null space of $A^\\top$?\n",
    "    7. Find a basis for the null space of $A$.\n",
    "    8. Consider the system $Ax = b$ where $b$ is given by the vectors below. Are there\n",
    "    \n",
    "        (i) infinitely many solutions\n",
    "        \n",
    "        (ii) one unique solution\n",
    "        \n",
    "        (iii) no solutions\n",
    "        \n",
    "        (iv) not enough information\n",
    "        \n",
    "        1. $$b=\\begin{bmatrix}2\\\\-2\\\\0\\end{bmatrix}$$\n",
    "        \n",
    "        2. $$b=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$$\n",
    "        \n",
    "        3. $$b=\\begin{bmatrix}0\\\\1\\\\0\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0abe4",
   "metadata": {},
   "source": [
    "A.\n",
    "\n",
    "We do Gauss-Jordan on $A$ (by hand, no `sympy` on final!). The nonzero rows in its RREF are a basis for the row space.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "1 & -1 & -2 & 4\\\\\n",
    "-2 & 1 & 3 & -7\\\\\n",
    "1 & 0 & -1 & 3\n",
    "\\end{bmatrix}\n",
    "    &\\underrightarrow{R_2 \\leftarrow 2 R_1 + R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & -2 & 4\\\\\n",
    "    0 & -1 & -1 & 1\\\\\n",
    "    1 & 0 & -1 & 3\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_3 \\leftarrow - R_1 + R_3}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & -2 & 4\\\\\n",
    "    0 & -1 & -1 & 1\\\\\n",
    "    0 & 1 & 1 & -1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_3 \\leftarrow R_2 + R_3}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & -2 & 4\\\\\n",
    "    0 & -1 & -1 & 1\\\\\n",
    "    0 & 0 & 0 & 0\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_1 \\leftarrow R_1 - R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -1 & 3\\\\\n",
    "    0 & -1 & -1 & 1\\\\\n",
    "    0 & 0 & 0 & 0\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_2 \\leftarrow -R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -1 & 3\\\\\n",
    "    0 & 1 & 1 & -1\\\\\n",
    "    0 & 0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So a basis for the row space of $A$ is\n",
    "$$\n",
    "\\left\\{\\begin{bmatrix}1\\\\0\\\\-1\\\\3\\end{bmatrix}, \\begin{bmatrix}0\\\\1\\\\1\\\\-1\\end{bmatrix}\\right\\}\n",
    "$$\n",
    "(it doesn't matter whether you write them as row vectors or column vectors, I'm just using the convention that we usuall write vectors as column vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf11aef",
   "metadata": {},
   "source": [
    "B.\n",
    "\n",
    "We do Gauss-Jordan on $A^\\top$ (by hand, no `sympy` on final!). The nonzero rows in its RREF are a basis for the column space.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "1 & -2 & 1 \\\\\n",
    "-1 & 1 & 0 \\\\\n",
    "-2 & 3 & -1 \\\\\n",
    "4 & -7 & 3\n",
    "\\end{bmatrix}\n",
    "    &\\underrightarrow{R_2 \\leftarrow R_1 + R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -2 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    -2 & 3 & -1 \\\\\n",
    "    4 & -7 & 3\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_3 \\leftarrow 2 R_1 + R_3}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -2 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    4 & -7 & 3\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_4 \\leftarrow -4 R_1 + R_4}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -2 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    0 & 1 & -1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_3 \\leftarrow - R_2 + R_3}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -2 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 1 & -1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_4 \\leftarrow R_2 + R_4}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -2 & 1 \\\\\n",
    "    0 & -1 & 1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_2 \\leftarrow - R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -2 & 1 \\\\\n",
    "    0 & 1 & -1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_1 \\leftarrow R_1 + 2 R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -1 \\\\\n",
    "    0 & 1 & -1 \\\\\n",
    "    0 & 0 & 0 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So a basis for the column space of $A$ is\n",
    "$$\n",
    "\\left\\{ \\begin{bmatrix}1\\\\0\\\\-1\\end{bmatrix},  \\begin{bmatrix}0\\\\1\\\\-1\\end{bmatrix}\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a4d3e",
   "metadata": {},
   "source": [
    "C.\n",
    "\n",
    "The range is the same as the column space. So a basis for the range of $A$ is \n",
    "$$\n",
    "\\left\\{ \\begin{bmatrix}1\\\\0\\\\-1\\end{bmatrix},  \\begin{bmatrix}0\\\\1\\\\-1\\end{bmatrix}\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadfbb1",
   "metadata": {},
   "source": [
    "D.\n",
    "\n",
    "The rank of $A$ can be computed using any of these techniques:\n",
    "1. The number of linearly independent rows of $A$\n",
    "2. The number of linearly independent columns of $A$\n",
    "3. The number of pivots in the RREF of $A$\n",
    "4. The dimension of the row space\n",
    "5. The dimension of the column space/range\n",
    "6. The number of nonzero singular values of $A$\n",
    "\n",
    "Remember:\n",
    "\n",
    "- A pivot is a column (or row) in the RREF that has a leading 1.\n",
    "- The dimension of a subspace is the length of a basis.\n",
    "- The singular values are the largest $\\min(m, n)$ eigenvalues of $AA^\\top$ or $A^\\top A$.\n",
    "    \n",
    "We've effectively computed the first five options. So $\\mathrm{rank}(A) = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01f3de",
   "metadata": {},
   "source": [
    "E.\n",
    "\n",
    "The rank-nullity theorem states that\n",
    "$$\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n$$\n",
    "where $n$ is the number of columns of $A$, and $\\mathrm{nullity}(A)$ is the dimension of the row space. Since $\\mathrm{rank}(A) = 2$, and there are $4$ columns, $\\mathrm{nullity}(A) = 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957946f6",
   "metadata": {},
   "source": [
    "F. \n",
    "\n",
    "We could use the rank-nullity theorem on $A^\\top$:\n",
    "$$\\mathrm{rank}(A^\\top) + \\mathrm{nullity}(A^\\top) = m$$\n",
    "where $m$ is the number of rows of $A$ (i.e., the number of columns of $A^\\top$). Since the rows of $A^\\top$ are the columns of $A$ (and vice versa), looking at many of the ways to compute rank shows that $\\mathrm{rank}(A^\\top) = \\mathrm{rank}(A)$. So since there are $3$ rows of $A$, and $\\mathrm{rank}(A) = 2$, the dimension of the null space of $A^\\top$ is $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83ec6b",
   "metadata": {},
   "source": [
    "G.\n",
    "\n",
    "The definition of the null space of $A$ is the set of solutions to $Ax = 0$. In set notation, we write this as\n",
    "$$\n",
    "\\mathrm{null}(A) = \\left\\{x \\in \\mathbb{R}^n : Ax = 0\\right\\}\n",
    "$$\n",
    "We have to solve the system\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & -1 & -2 & 4\\\\\n",
    "-2 & 1 & 3 & -7\\\\\n",
    "1 & 0 & -1 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\x_2\\\\x_3\\\\x_4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0\\\\0\\\\0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "We should be expecting infinitely many solutions (why?), so we have to solve this using Gauss-Jordan on the augmented matrix\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & -1 & -2 & 4 &\\mid& 0\\\\\n",
    "-2 & 1 & 3 & -7 &\\mid& 0\\\\\n",
    "1 & 0 & -1 & 3 &\\mid& 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we do the exact same steps for finding the RREF of $A$ as in part (A) on this augmented matrix, we will get the same result, and the zeros won't change (no row operations can change them from zero). This leaves us with the augmented matrix\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    1 & 0 & -1 & 3 & \\mid& 0\\\\\n",
    "    0 & 1 & 1 & -1 & \\mid& 0\\\\\n",
    "    0 & 0 & 0 & 0 & \\mid& 0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Since we only have $3$ rows for four variables, $x_4$ will always be free. Since we have a row of all zeros, $x_3$ is also free. The first two equations tell us\n",
    "$$\n",
    "x_1 - x_3 + 3x_4 = 0 \\implies x_1 = x_3 - 3x_4\n",
    "$$\n",
    "and\n",
    "$$\n",
    "x_2 + x_3 - x_4 = 0 \\implies x_2 = -x_3 + x_4.\n",
    "$$\n",
    "So the null space is the set of all $x$ in the form\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\x_2\\\\x_3\\\\x_4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_3 - 3x_4\\\\-x_3 + x_4\\\\x_3\\\\x_4\n",
    "\\end{bmatrix}\\quad\\text{for any $x_3, x_4 \\in \\mathbb{R}$.}\n",
    "$$\n",
    "\n",
    "If we separate this into a linear combination of two vectors, we get\n",
    "$$\n",
    "x =\n",
    "\\begin{bmatrix}\n",
    "x_3 - 3x_4\\\\-x_3 + x_4\\\\x_3\\\\x_4\n",
    "\\end{bmatrix}\n",
    "=\n",
    "x_3 \\begin{bmatrix}1 \\\\-1\\\\1\\\\0\\end{bmatrix}\n",
    "+\n",
    "x_4 \\begin{bmatrix}-3\\\\1\\\\0\\\\1\\end{bmatrix}\n",
    "\\quad\\text{for any $x_3, x_4 \\in \\mathbb{R}$.}\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\left\\{\\begin{bmatrix}1 \\\\-1\\\\1\\\\0\\end{bmatrix}, \\begin{bmatrix}-3\\\\1\\\\0\\\\1\\end{bmatrix}\\right\\}\n",
    "$$\n",
    "are a spanning set for the null space of $A$. Since we know the dimension of the null space of $A$ is 2, this must also be a basis for the null space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68281a67",
   "metadata": {},
   "source": [
    "H.\n",
    "\n",
    "Since this matrix has a null space that is more than just the zero vector, we know immediately that it never has a unique solution for any system.\n",
    "\n",
    "Why? Pick a nonzero vector in the null space and call it $x_0$. This means that $Ax_0 = 0$. Now, if $x$ is any solution to $Ax = b$, we have\n",
    "$$A(x + x_0) = Ax + Ax_0 = Ax = b.$$\n",
    "So $x + x_0$ is also a solution. Since there are infinitely many nonzero vectors in the null space (i.e., any nonzero linear combination of the basis in part (G)), there are infinitely many solutions to any system that has one solution.\n",
    "\n",
    "So we will never answer with (ii).\n",
    "\n",
    "a. Again, `numpy` won't work for us since $A$ isn't square. We can do Gauss-Jordan on the augmented system $[A \\mid b]$, and this will tell us if there's a solution. It's good practice, so we'll try it in part (c) below. But we'll do it another way here.\n",
    "\n",
    "Asking whether $Ax = b$ has a solution is the same as asking whether $b$ is in the range of $A$ (why?). We have a basis of the range of $A$, so this is the same as asking if there's a linear combination\n",
    "$$\n",
    "c_1\\begin{bmatrix}1\\\\0\\\\-1\\end{bmatrix} + c_2\\begin{bmatrix}0\\\\1\\\\-1\\end{bmatrix} = \\begin{bmatrix}2 \\\\ -2 \\\\ 0\\end{bmatrix}.\n",
    "$$\n",
    "By inspection, we can see that for the first line to work out, $c_1 = 2$ and for the second line to work out, $c_2 = 2$. The last line in that case would be $2 \\times -1 + 2 \\times 1 = 0$ which is true! So there is a linear combination that works, $b$ is in the range of $A$, and the system has infinitely many solutions. So (i).\n",
    "\n",
    "Interestingly, we didn't actually compute a solution for $Ax = b$, but we know there are infinitely many. Can you figure out how to get a solution without Gauss-Jordan? (Hint: pseudoinverses work in more than just overdetermined cases...)\n",
    "\n",
    "b. The null space is the set of all solutions to this problem. Any linear combination of the basis vectors in part (G) are a solution. Since there are infinitely many linear combinations, there are infinitely many solutions. So (i).\n",
    "\n",
    "c. As suggested in (a) above, let's do Gauss-Jordan. It will be the exact same steps as computing the basis of the row space, we just take the right hand side along for the ride:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{bmatrix}\n",
    "1 & -1 & -2 & 4 & \\mid & 0\\\\\n",
    "-2 & 1 & 3 & -7 & \\mid & 1\\\\\n",
    "1 & 0 & -1 & 3 & \\mid & 0\n",
    "\\end{bmatrix}\n",
    "    &\\underrightarrow{R_2 \\leftarrow 2 R_1 + R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & -2 & 4 & \\mid & 0\\\\\n",
    "    0 & -1 & -1 & 1 & \\mid & 1\\\\\n",
    "    1 & 0 & -1 & 3 & \\mid & 0\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_3 \\leftarrow - R_1 + R_3}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & -2 & 4 & \\mid & 0\\\\\n",
    "    0 & -1 & -1 & 1 & \\mid & 1\\\\\n",
    "    0 & 1 & 1 & -1 & \\mid & 0\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_3 \\leftarrow R_2 + R_3}\n",
    "    \\begin{bmatrix}\n",
    "    1 & -1 & -2 & 4 & \\mid & 0\\\\\n",
    "    0 & -1 & -1 & 1 & \\mid & 1\\\\\n",
    "    0 & 0 & 0 & 0 & \\mid & 1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_1 \\leftarrow R_1 - R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -1 & 3 & \\mid & -1\\\\\n",
    "    0 & -1 & -1 & 1 & \\mid & 1\\\\\n",
    "    0 & 0 & 0 & 0 & \\mid & 1\n",
    "    \\end{bmatrix}\\\\\n",
    "    &\\underrightarrow{R_2 \\leftarrow -R_2}\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 & -1 & 3 & \\mid & -1\\\\\n",
    "    0 & 1 & 1 & -1 & \\mid & -1\\\\\n",
    "    0 & 0 & 0 & 0 & \\mid & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The last row would require that $0 = 1$. This isn't possible, so there are no solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66889ef",
   "metadata": {},
   "source": [
    "---\n",
    "##### This question reviews least squares, the pseudoinverse, column space, projection matrices, and (optionally) plotting\n",
    "5. Consider the list of $(x, y)$ points\n",
    "$$\\begin{matrix}\n",
    "(-0.5, & 0.1)\\\\\n",
    "(-1, & 2)\\\\\n",
    "(1, & 2)\\\\\n",
    "(2, & 5)\\\\\n",
    "(2.5, & 6)\\\\\n",
    "\\end{matrix}\n",
    "$$\n",
    "    1. Come up with an overdetermined system $Ax = b$ whose least squares solution $c = [c_0, c_1, c_2]^\\top$ gives the quadratic of best fit $y = c_0 + c_1 x + c_2 x^2$ through these points.\n",
    "    2. Solve this system using the normal equations.\n",
    "    3. Solve this system using the left inverse.\n",
    "    4. Write the matrix that projects a vector in $\\mathbb{R}^5$ onto the column space of $A$.\n",
    "    5. Check that your least squares solution solves $Ac = \\mathrm{proj}_{\\mathrm{col}(A)}(b)$.\n",
    "    6. (Optional) Plot the points and the quadratic of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8ee51",
   "metadata": {},
   "source": [
    "A.\n",
    "\n",
    "These 5 points give us five equations for three unknowns:\n",
    "$$\n",
    "\\begin{matrix}\n",
    "c_0 &+ & c_1 \\cdot (-0.5) &+ & c_2 \\cdot (-0.5)^2 &= 0.1 \\\\\n",
    "c_0 &+ & c_1 \\cdot (-1) &+ & c_2 \\cdot (-1)^2 &= 2 \\\\\n",
    "c_0 &+ & c_1 \\cdot 1 &+ & c_2 \\cdot 1^2 &= 2 \\\\\n",
    "c_0 &+ & c_1 \\cdot 2 &+ & c_2 \\cdot 2^2 &= 5 \\\\\n",
    "c_0 &+ & c_1 \\cdot 2.5 &+ & c_2 \\cdot 2.5^2 &= 6\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "In matrix form:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & -0.5 & (-0.5)^2\\\\\n",
    "1 & -1 & (-1)^2\\\\\n",
    "1 & 1 & 1^2\\\\\n",
    "1 & 2 & 2^2\\\\\n",
    "1 & 2.5 & 2.5^2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "c_0 \\\\ c_1 \\\\c_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "2 \\\\\n",
    "2\\\\\n",
    "5\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & -0.5 & (-0.5)^2\\\\\n",
    "1 & -1 & (-1)^2\\\\\n",
    "1 & 1 & 1^2\\\\\n",
    "1 & 2 & 2^2\\\\\n",
    "1 & 2.5 & 2.5^2\n",
    "\\end{bmatrix}, \\qquad\n",
    "b = \\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "2 \\\\\n",
    "2\\\\\n",
    "5\\\\\n",
    "6\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0bc5fd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.82697495],\n",
       "       [0.20635838],\n",
       "       [0.81117534]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B\n",
    "\n",
    "A = np.ones((5, 3))\n",
    "x = np.array([-0.5, -1, 1, 2, 2.5])\n",
    "A[:, 1] = x\n",
    "A[:, 2] = x ** 2\n",
    "\n",
    "b = np.array([[0.1, 2, 2, 5, 6]]).T\n",
    "\n",
    "c = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7974201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.82697495]\n",
      " [0.20635838]\n",
      " [0.81117534]]\n"
     ]
    }
   ],
   "source": [
    "# C\n",
    "\n",
    "A_left_inv = np.linalg.inv(A.T @ A) @ A.T\n",
    "c = A_left_inv @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e75b0d5",
   "metadata": {},
   "source": [
    "D.\n",
    "\n",
    "The least squares solution is the vector $c$ so that $\\|Ac - b\\|$ is minimized. This is the same as saying that $Ac$ is the vector in the range/column space of $A$ closest to $b$. Thus, $Ac$ is actually the projection of $b$ onto the column space of $A$.\n",
    "\n",
    "We can find $c$ by applying the left inverse to $b$:\n",
    "\n",
    "$$c = (A^\\top A)^{-1} A.T b \\implies \\mathrm{proj}_{\\mathrm{col}(A)}(b) = Ac = A (A^\\top A)^{-1} A^\\top b.$$\n",
    "\n",
    "For any vector in $\\mathbb{R}^5$ then, the projection matrix onto the range of $A$ is\n",
    "$$A (A^\\top A)^{-1} A^\\top$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8d98202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4026975   0.38728324  0.27552987  0.04624277 -0.11175337]\n",
      " [ 0.38728324  0.74566474 -0.15606936 -0.07514451  0.0982659 ]\n",
      " [ 0.27552987 -0.15606936  0.71483622  0.29479769 -0.12909441]\n",
      " [ 0.04624277 -0.07514451  0.29479769  0.36416185  0.3699422 ]\n",
      " [-0.11175337  0.0982659  -0.12909441  0.3699422   0.77263969]]\n"
     ]
    }
   ],
   "source": [
    "# D\n",
    "proj_mat = A @ np.linalg.inv(A.T @ A) @ A.T\n",
    "print(proj_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "164231c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E\n",
    "np.allclose(A @ c, proj_mat @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a8d0d971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f91a20c9d30>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x, b.ravel())  # Need b as a 1d array\n",
    "x_quad = np.linspace(min(x), max(x))\n",
    "y_quad = c[0] + c[1] * x_quad + c[2] * (x_quad ** 2)\n",
    "plt.plot(x_quad, y_quad, 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
